
lister les environnement les versions -  VIM  et PBS solution de sauvegarde - console centralisÃ©
facilitÃ© d'administration => fixer les critÃ©res 
sauvegarde faciliter d'amdin, cout, pourfaire quoi, ect(une vrai reflexion)



# ğŸŒ **SynthÃ¨se comparative â€” VMware ESXi, Proxmox VE et Incus**
*Comparaison technique des trois solutions Ã©tudiÃ©es dans le cadre du projet.*

Lâ€™infrastructure initialement envisagÃ©e reposait sur **VMware ESXi**, hyperviseur rÃ©putÃ© pour sa **stabilitÃ©** et son **adoption dans le monde professionnel**.  
Au cours du projet, deux solutions alternatives ont Ã©tÃ© Ã©tudiÃ©es et dÃ©ployÃ©es : **Proxmox VE**, environnement dâ€™hypervision libre intÃ©grant **virtualisation complÃ¨te**, **conteneurs** et **capacitÃ©s de clustering avancÃ©es**, et **Incus**, plateforme moderne de **conteneurisation systÃ¨me** dÃ©rivÃ©e de LXD.

Cette synthÃ¨se analyse les **fonctionnements**, **forces**, **limites** et **domaines dâ€™usage** de ces trois technologies, afin de comprendre ce que chacune peut apporter â€” ou non â€” Ã  une architecture virtuelle.

---

## **1. VMware ESXi â€” Un hyperviseur professionnel robuste et Ã©prouvÃ©**

VMware ESXi repose sur une architecture **bareâ€‘metal**, oÃ¹ lâ€™hyperviseur est installÃ© directement sur le serveur **sans systÃ¨me dâ€™exploitation intermÃ©diaire**.  
Son fonctionnement repose exclusivement sur la **virtualisation complÃ¨te** via le moteur **VMkernel**, reconnu pour sa **stabilitÃ©** et son **efficacitÃ©**.

ESXi se distingue par une excellente **gestion des ressources**, une grande **fiabilitÃ©**, et une intÃ©gration serrÃ©e avec lâ€™Ã©cosystÃ¨me VMware (**vCenter**, **vMotion**, **VSAN**, **HA**â€¦).  
Cependant, bon nombre de fonctionnalitÃ©s avancÃ©es â€” telles que la **haute disponibilitÃ©**, le **clustering**, ou la **migration Ã  chaud** â€” nÃ©cessitent **vCenter**, une solution **payante**.

La dÃ©pendance aux **licences** et lâ€™Ã©cosystÃ¨me **propriÃ©taire** limitent la flexibilitÃ© et augmentent le coÃ»t dâ€™adoption, en particulier pour des environnements Ã©ducatifs ou expÃ©rimentaux.

**ESXi offre donc une plateforme extrÃªmement solide, mais parfois rigide et coÃ»teuse.**

---

## **2. Proxmox VE â€” Une alternative complÃ¨te, ouverte et polyvalente**

Proxmox VE combine dans un mÃªme environnement :

- un hyperviseur **KVM** pour la virtualisation complÃ¨te,  
- **LXC** pour la virtualisation lÃ©gÃ¨re,  
- la gestion native du stockage **ZFS**,  
- un **clustering intÃ©grÃ©**,  
- lâ€™utilisation directe de **Ceph** pour le stockage distribuÃ©.

Son fonctionnement repose entiÃ¨rement sur des technologies **openâ€‘source** Ã©prouvÃ©es.  
Contrairement Ã  ESXi, Proxmox ne nÃ©cessite **aucun composant propriÃ©taire** pour activer le **clustering**, la **migration de VM** ou la **haute disponibilitÃ©**.

L'interface web est **moderne**, **centralisÃ©e** et **intuitive**, ce qui facilite la gestion de plusieurs nÅ“uds.  
La flexibilitÃ© de Proxmox est un atout : stockage hybride, intÃ©gration rÃ©seau avancÃ©e, gestion fine des conteneurs comme des machines virtuelles.

Sa seule limite rÃ©elle rÃ©side dans une **consommation de ressources plus Ã©levÃ©e** que celle dâ€™un hyperviseur spÃ©cialisÃ©, ainsi quâ€™une architecture parfois plus **complexe** Ã  maÃ®triser lorsque lâ€™on combine **cluster**, **Ceph** et **ZFS**.

---

## **3. Incus â€” Conteneurisation systÃ¨me performante et flexible**

Incus est un systÃ¨me de **system containers**, orientÃ© vers la **virtualisation lÃ©gÃ¨re**.  
Contrairement Ã  ESXi et Proxmox, Incus ne propose **pas de virtualisation complÃ¨te** : les conteneurs partagent **le noyau de lâ€™hÃ´te**.

Cette approche permet une **efficacitÃ© exceptionnelle** :  
- consommation **minimale** de ressources,  
- dÃ©marrage **instantanÃ©**,  
- dÃ©ploiement **rapide en sÃ©rie**,  
- snapshots **quasi immÃ©diats**.

Incus propose Ã©galement des fonctionnalitÃ©s avancÃ©es comme la **gestion de profils**, la **configuration rÃ©seau intÃ©grÃ©e**, les **images cloudâ€‘init**, et le **clustering multiâ€‘nÅ“uds**.

En revanche, Incus ne peut pas exÃ©cuter des systÃ¨mes nÃ©cessitant **leur propre kernel** (Windows, appliances virtuelles, etc.).  
Il ne remplace donc pas un **hyperviseur classique**.

Son usage est idÃ©al pour des environnements **DevOps**, des environnements de **test**, ou des dÃ©ploiements massifs de conteneurs systÃ¨me, mais pas pour des workloads nÃ©cessitant une **isolation matÃ©rielle complÃ¨te**.


## Comparatif gÃ©nÃ©ral des fonctionnalitÃ©s

| CritÃ¨re                | **VMware ESXi**                                   | **Proxmox VE**                   | **Incus**                        |
| ---------------------- | ------------------------------------------------- | -------------------------------- | -------------------------------- |
| Type de virtualisation | **Virtualisation complÃ¨te**                       | **KVM (VM) + LXC (conteneurs)**  | **Conteneurs systÃ¨me**           |
| Licence                | PropriÃ©taire (payant pour les fonctions avancÃ©es) | **Openâ€‘source**                  | **Openâ€‘source**                  |
| Clustering             | âœ” Avec vCenter (payant)                           | **âœ” IntÃ©grÃ© nativement**         | âœ” Clustering conteneurs          |
| Migration Ã  chaud      | âœ” vMotion (payant)                                | **âœ” Native, gratuite**           | N/A (pas de VM)                  |
| Stockage distribuÃ©     | âœ” VSAN (licence)                                  | **âœ” Ceph intÃ©grÃ©**               | Support externe possible         |
| Haute disponibilitÃ©    | âœ” Avec vCenter                                    | **âœ” Proxmox HA (Ceph)**          | LimitÃ© aux conteneurs            |
| Performances           | Excellentes                                       | TrÃ¨s bonnes                      | **Exceptionnelles (conteneurs)** |
| FlexibilitÃ©            | Moyenne (Ã©cosystÃ¨me fermÃ©)                        | **TrÃ¨s Ã©levÃ©e**                  | TrÃ¨s Ã©levÃ©e                      |
| Support OS invitÃ©s     | Tous OS                                           | Tous OS                          | Linux uniquement (mÃªme kernel)   |
| IdÃ©al pour             | Production pro propriÃ©taire                       | **Infra pro/Ã©ducative complÃ¨te** | DevOps, labs, conteneurs         |


---



# Document Technique - Incus 
# 1. Prise en main physique

La mise en service du serveur a commencÃ© par une intervention physique indispensable. Le module ILO, normalement utilisÃ© pour administrer la machine Ã  distance, Ã©tait verrouillÃ© car les identifiants dâ€™accÃ¨s avaient Ã©tÃ© perdus. Jâ€™ai donc connectÃ© un clavier, un Ã©cran et un cÃ¢ble rÃ©seau directement sur la machine afin dâ€™accÃ©der au BIOS.

Depuis lâ€™interface de gestion, jâ€™ai procÃ©dÃ© Ã  la rÃ©initialisation complÃ¨te des identifiants ILO, rendant Ã  nouveau possible lâ€™administration distante sÃ©curisÃ©e. Cette Ã©tape a permis de reprendre le contrÃ´le matÃ©riel de lâ€™Ã©quipement avant toute installation logicielle. En modifiant directement les identifiants dans le bios.



# 2. IntÃ©gration dâ€™ILO dans lâ€™infrastructure rÃ©seau

Lâ€™objectif suivant consistait Ã  intÃ©grer le serveur dans le rÃ©seau pÃ©dagogique, construit sur le plan dâ€™adressage 10.202.0.0/16.
Le premier serveur Proxmox, dÃ©jÃ  opÃ©rationnel, utilisait lâ€™adresse 10.202.4.1. Afin de suivre une logique cohÃ©rente, jâ€™ai configurÃ© ILO avec lâ€™adresse 10.202.4.2/16.

Une fois lâ€™interface rÃ©seau ILO paramÃ©trÃ©e, lâ€™administration Ã  distance via un navigateur est devenue possible Ã  lâ€™adresse suivante :
https://10.202.4.2



# 3. RÃ©initialisation complÃ¨te des volumes RAID

Avant dâ€™installer un nouveau systÃ¨me, il Ã©tait impÃ©ratif de nettoyer lâ€™infrastructure de stockage. Le serveur contenait plusieurs volumes RAID matÃ©riels, susceptibles dâ€™interfÃ©rer avec lâ€™installation ou de gÃ©nÃ©rer des conflits de partitions.

Jâ€™ai supprimÃ© successivement tous les volumes RAID depuis lâ€™outil de gestion dÃ©diÃ© dans le bios, et rÃ©initialisÃ© chaque disque. Cette opÃ©ration garantit un environnement de stockage sain et parfaitement maÃ®trisÃ© pour accueillir Ubuntu Server avec Incus.



# 4. Installation dâ€™Ubuntu Server via lâ€™interface ILO

Lâ€™installation dâ€™Ubuntu Server a Ã©tÃ© rÃ©alisÃ©e entiÃ¨rement Ã  distance grÃ¢ce Ã  la fonctionnalitÃ© Virtual Media dâ€™ILO.
Jâ€™ai tÃ©lÃ©chargÃ© lâ€™image ISO depuis mon poste, lâ€™ai montÃ©e virtuellement dans le serveur, puis forcÃ© le boot sur ce support.

Lâ€™assistant dâ€™installation dâ€™Ubuntu a Ã©tÃ© configurÃ© pour utiliser un seul disque avec une structure moderne composÃ©e dâ€™une partition EFI et dâ€™un systÃ¨me racine /. Ce schÃ©ma assure une compatibilitÃ© optimale avec UEFI et permet une maintenance simplifiÃ©e.


# 5. Configuration rÃ©seau du systÃ¨me Ubuntu

Une fois Ubuntu installÃ©, jâ€™ai configurÃ© son interface rÃ©seau en adressage statique afin de lâ€™intÃ©grer pleinement Ã  lâ€™infrastructure existante.
Le serveur Ubuntu utilise dÃ©sormais lâ€™adresse 10.202.4.69/16, distincte de lâ€™adresse ILO pour garantir un cloisonnement logique entre :

la gestion matÃ©rielle (ILO),

et lâ€™administration du systÃ¨me dâ€™exploitation.

La passerelle a Ã©tÃ© fixÃ©e sur 10.202.4.1, et les serveurs DNS configurÃ©s pour assurer une rÃ©solution fiable.
Cette configuration mâ€™a permis dâ€™accÃ©der au serveur via SSH, ce qui a marquÃ© la transition vers lâ€™administration entiÃ¨rement distante.



# 6. Installation et initialisation du dÃ©mon Incus

Avec un serveur fonctionnel, jâ€™ai installer Incus, un moteur de virtualisation moderne basÃ© sur LXC/LXD.
Incus nâ€™Ã©tant pas disponible dans les dÃ©pÃ´ts Ubuntu par dÃ©faut, jâ€™ai ajoutÃ© le dÃ©pÃ´t officiel Zabbly, permettant de rÃ©cupÃ©rer une version maintenue et stable du dÃ©mon.

AprÃ¨s installation, la commande dâ€™initialisation a automatisÃ© la crÃ©ation du stockage, du pont rÃ©seau virtuel et des paramÃ¨tres de sÃ©curitÃ©. Ã€ lâ€™issue de cette phase, Incus Ã©tait opÃ©rationnel et prÃªt Ã  hÃ©berger des conteneurs ou des machines virtuelles.


# 7. Mise en place de lâ€™interface graphique sÃ©curisÃ©e Incus UI

Bien quâ€™Incus puisse Ãªtre entiÃ¨rement pilotÃ© via la ligne de commande, jâ€™ai choisi dâ€™installer Incus UI, une interface web qui amÃ©liore considÃ©rablement la gestion des ressources virtuelles.

Incus UI fonctionne exclusivement en HTTPS, ce qui a nÃ©cessitÃ© la gÃ©nÃ©ration dâ€™un certificat serveur (.crt) et son ajout au trust store dâ€™Incus. Une seconde Ã©tape a consistÃ© Ã  importer un certificat utilisateur (.pfx) dans mon navigateur afin dâ€™Ã©tablir une connexion sÃ©curisÃ©e et authentifiÃ©e.

Cette mÃ©canique de certificats permet dâ€™assurer un haut niveau de sÃ©curitÃ©, alignÃ© avec les pratiques des environnements professionnels.



# 8. AccÃ¨s sÃ©curisÃ© Ã  lâ€™interface Incus UI

Une fois les certificats installÃ©s et le pare-feu reconfigurÃ© pour autoriser les communications sur le port 8443, lâ€™interface graphique est devenue accessible via :
https://10.202.4.69:8443/ui

Cela a marquÃ© la transition vers une gestion visuelle complÃ¨te, facilitant la crÃ©ation, le suivi et la maintenance des instances.



# 9. CrÃ©ation des premiÃ¨res instances virtuelles

Depuis Incus UI, jâ€™ai crÃ©Ã© mes premiÃ¨res instances afin de valider le fonctionnement de la plateforme :

deux conteneurs AlmaLinux, adaptÃ©s aux environnements de type entreprise,

un conteneur Devuan, reconnu pour sa lÃ©gÃ¨retÃ© et lâ€™absence de systemd,

une machine virtuelle Linux Mint, permettant de tester une interface graphique complÃ¨te.

Chaque instance a Ã©tÃ© configurÃ©e avec un stockage dÃ©diÃ©, un rÃ©seau virtuel stable et des ressources adaptÃ©es. La mise en Å“uvre sâ€™est rÃ©vÃ©lÃ©e fluide grÃ¢ce Ã  la structure modulaire dâ€™Incus.


# 10. Architecture finale du systÃ¨me

Lâ€™architecture globale dÃ©ployÃ©e reflÃ¨te une infrastructure moderne, sÃ©curisÃ©e et segmentÃ©e, oÃ¹ chaque niveau â€” matÃ©riel, OS, dÃ©mon de virtualisation, interface graphique et instances â€” coopÃ¨re de maniÃ¨re cohÃ©rente.

                   RÃ‰SEAU IUT
             (Modem / Routeur IUT)
                         â”‚
                         â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  SWITCH   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–²        â–²
         10.202.x.x â”‚        â”‚ 10.202.4.x
                    â”‚        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    PC        â”‚   â”‚   SERVEUR INCUS     â”‚
        â”‚  (SSH / ILO) â”‚   â”‚ Ubuntu + Incus + UI â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


# Conclusion professionnelle

Ce projet mâ€™a permis de dÃ©ployer une solution de virtualisation complÃ¨te, depuis la remise en service matÃ©rielle du serveur jusquâ€™Ã  la gestion avancÃ©e via une interface web sÃ©curisÃ©e.
Jâ€™ai configurÃ© lâ€™ensemble de lâ€™infrastructure rÃ©seau, rÃ©initialisÃ© les systÃ¨mes RAID, installÃ© Ubuntu Server, mis en place Incus et son interface graphique, puis crÃ©Ã© mes premiÃ¨res instances en conteneurs et en VM.

Ce travail mâ€™a confrontÃ© Ã  des problÃ©matiques rÃ©elles dâ€™administration systÃ¨me : gestion du matÃ©riel, configuration rÃ©seau, sÃ©curitÃ© HTTPS, orchestration virtuelle.
Le rÃ©sultat est une plateforme propre, performante, professionnelle et totalement exploitable pour un usage pÃ©dagogique ou applicatif.
